{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e16ca5-1d5d-4826-bc33-4487a68569e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import string\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from random import randint\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6245d02f-9d94-4e9e-b9bf-30b1d1b754e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/kapil/Desktop/NLP.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADITIS~1\\AppData\\Local\\Temp/ipykernel_18300/657821443.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading text into memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/kapil/Desktop/NLP.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/kapil/Desktop/NLP.txt'"
     ]
    }
   ],
   "source": [
    "# Loading text into memory\n",
    "file= open('C:/Users/kapil/Desktop/NLP.txt', 'r')\n",
    "text= file.read()\n",
    "file.close()\n",
    "print(text[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f18fa49-d920-4995-821d-28cb3cf42482",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ADITIS~1\\AppData\\Local\\Temp/ipykernel_18300/2978554908.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mtokens\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleaning text\n",
    "def clean_text(text):\n",
    "    # replace '--' with a space\n",
    "    text= text.replace('--', ' ')\n",
    "    #splitting into tokens by space\n",
    "    tokens= text.split()\n",
    "    #removing punctuations\n",
    "    tokens= [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "    #removing non-aplhabetic tokens\n",
    "    tokens= [word for word in tokens if word.isalpha()]\n",
    "    # lower-case\n",
    "    tokens= [word.lower() for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "tokens= clean_text(text)\n",
    "print(len(tokens))\n",
    "print(len(set(tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0dbf54b7-d3ba-4494-b05c-371802c72af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47896\n"
     ]
    }
   ],
   "source": [
    "# creating sequences from tokens\n",
    "length= 10 + 1\n",
    "sequences= list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq= tokens[i-length:i]\n",
    "    line= ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "789e9bdd-add2-4974-ad0b-e0e90d5be26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving into file\n",
    "data = '\\n'.join(sequences)\n",
    "file = open('Poirot.txt', 'w')\n",
    "file.write(data)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c4f5d8b-1325-4617-b88a-0f56da7795de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading Poirot.txt file\n",
    "file= open('C:/Users/kapil/Desktop\\Poirot.txt', 'r')\n",
    "doc= file.read()\n",
    "file.close()\n",
    "lines= doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93a255b9-43fd-406e-b8ea-90ba52612fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d86741e4-e272-407e-9a34-e5650609cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5596\n"
     ]
    }
   ],
   "source": [
    "# Size of vocabolary\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99ce6882-20ac-47b3-ba6e-00b4232cf68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# Separating into input, output sequences\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "\n",
    "# One-hot encoding output\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "print(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a43c2d2-8264-41d1-b944-3b4a31988172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 10, 10)            55960     \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 10, 200)           168800    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 200)               320800    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               40200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5596)              1124796   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,710,556\n",
      "Trainable params: 1,710,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Builduing LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=seq_length))\n",
    "model.add(LSTM(200, return_sequences=True))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04d3401c-f02a-4c94-8946-22f15aed93f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "375/375 [==============================] - 373s 916ms/step - loss: 6.7283 - accuracy: 0.0617\n",
      "Epoch 2/100\n",
      "375/375 [==============================] - 333s 888ms/step - loss: 6.3614 - accuracy: 0.0621\n",
      "Epoch 3/100\n",
      "375/375 [==============================] - 335s 892ms/step - loss: 6.2157 - accuracy: 0.0702\n",
      "Epoch 4/100\n",
      "375/375 [==============================] - 329s 878ms/step - loss: 6.0562 - accuracy: 0.0750\n",
      "Epoch 5/100\n",
      "375/375 [==============================] - 330s 880ms/step - loss: 5.9447 - accuracy: 0.0786\n",
      "Epoch 6/100\n",
      "375/375 [==============================] - 319s 850ms/step - loss: 5.8471 - accuracy: 0.0834\n",
      "Epoch 7/100\n",
      "375/375 [==============================] - 320s 853ms/step - loss: 5.7447 - accuracy: 0.0878\n",
      "Epoch 8/100\n",
      "375/375 [==============================] - 318s 849ms/step - loss: 5.6504 - accuracy: 0.0904\n",
      "Epoch 9/100\n",
      "375/375 [==============================] - 320s 853ms/step - loss: 5.5628 - accuracy: 0.0945\n",
      "Epoch 10/100\n",
      "375/375 [==============================] - 320s 854ms/step - loss: 5.4777 - accuracy: 0.0983\n",
      "Epoch 11/100\n",
      "375/375 [==============================] - 319s 852ms/step - loss: 5.3899 - accuracy: 0.1021\n",
      "Epoch 12/100\n",
      "375/375 [==============================] - 320s 853ms/step - loss: 5.3030 - accuracy: 0.1059\n",
      "Epoch 13/100\n",
      "375/375 [==============================] - 322s 858ms/step - loss: 5.2109 - accuracy: 0.1092\n",
      "Epoch 14/100\n",
      "375/375 [==============================] - 319s 851ms/step - loss: 5.1160 - accuracy: 0.1118\n",
      "Epoch 15/100\n",
      "375/375 [==============================] - 319s 850ms/step - loss: 5.0267 - accuracy: 0.1162\n",
      "Epoch 16/100\n",
      "375/375 [==============================] - 319s 851ms/step - loss: 4.9348 - accuracy: 0.1198\n",
      "Epoch 17/100\n",
      "375/375 [==============================] - 321s 856ms/step - loss: 4.8395 - accuracy: 0.1218\n",
      "Epoch 18/100\n",
      "375/375 [==============================] - 320s 853ms/step - loss: 4.7433 - accuracy: 0.1261\n",
      "Epoch 19/100\n",
      "375/375 [==============================] - 321s 855ms/step - loss: 4.6463 - accuracy: 0.1281\n",
      "Epoch 20/100\n",
      "375/375 [==============================] - 318s 848ms/step - loss: 4.5469 - accuracy: 0.1318\n",
      "Epoch 21/100\n",
      "375/375 [==============================] - 323s 861ms/step - loss: 4.4454 - accuracy: 0.1367\n",
      "Epoch 22/100\n",
      "375/375 [==============================] - 321s 855ms/step - loss: 4.3487 - accuracy: 0.1419\n",
      "Epoch 23/100\n",
      "375/375 [==============================] - 322s 858ms/step - loss: 4.2544 - accuracy: 0.1485\n",
      "Epoch 24/100\n",
      "375/375 [==============================] - 323s 860ms/step - loss: 4.1632 - accuracy: 0.1565\n",
      "Epoch 25/100\n",
      "375/375 [==============================] - 323s 861ms/step - loss: 4.0698 - accuracy: 0.1658\n",
      "Epoch 26/100\n",
      "375/375 [==============================] - 323s 860ms/step - loss: 3.9831 - accuracy: 0.1753\n",
      "Epoch 27/100\n",
      "375/375 [==============================] - 322s 860ms/step - loss: 3.8993 - accuracy: 0.1850\n",
      "Epoch 28/100\n",
      "375/375 [==============================] - 321s 855ms/step - loss: 3.8248 - accuracy: 0.1947\n",
      "Epoch 29/100\n",
      "375/375 [==============================] - 321s 856ms/step - loss: 3.7437 - accuracy: 0.2042\n",
      "Epoch 30/100\n",
      "375/375 [==============================] - 323s 860ms/step - loss: 3.6692 - accuracy: 0.2160\n",
      "Epoch 31/100\n",
      "375/375 [==============================] - 326s 870ms/step - loss: 3.5983 - accuracy: 0.2242\n",
      "Epoch 32/100\n",
      "375/375 [==============================] - 322s 860ms/step - loss: 3.5286 - accuracy: 0.2370\n",
      "Epoch 33/100\n",
      "375/375 [==============================] - 325s 867ms/step - loss: 3.4639 - accuracy: 0.2453\n",
      "Epoch 34/100\n",
      "375/375 [==============================] - 325s 868ms/step - loss: 3.3989 - accuracy: 0.2551\n",
      "Epoch 35/100\n",
      "375/375 [==============================] - 325s 866ms/step - loss: 3.3407 - accuracy: 0.2615\n",
      "Epoch 36/100\n",
      "375/375 [==============================] - 326s 870ms/step - loss: 3.2787 - accuracy: 0.2701\n",
      "Epoch 37/100\n",
      "375/375 [==============================] - 323s 861ms/step - loss: 3.2136 - accuracy: 0.2830\n",
      "Epoch 38/100\n",
      "375/375 [==============================] - 324s 865ms/step - loss: 3.1551 - accuracy: 0.2910\n",
      "Epoch 39/100\n",
      "375/375 [==============================] - 322s 859ms/step - loss: 3.0943 - accuracy: 0.3022\n",
      "Epoch 40/100\n",
      "375/375 [==============================] - 321s 856ms/step - loss: 3.0396 - accuracy: 0.3110\n",
      "Epoch 41/100\n",
      "375/375 [==============================] - 322s 859ms/step - loss: 2.9850 - accuracy: 0.3192\n",
      "Epoch 42/100\n",
      "375/375 [==============================] - 321s 857ms/step - loss: 2.9312 - accuracy: 0.3292\n",
      "Epoch 43/100\n",
      "375/375 [==============================] - 324s 863ms/step - loss: 2.8775 - accuracy: 0.3376\n",
      "Epoch 44/100\n",
      "375/375 [==============================] - 323s 862ms/step - loss: 2.8276 - accuracy: 0.3472\n",
      "Epoch 45/100\n",
      "375/375 [==============================] - 323s 863ms/step - loss: 2.7723 - accuracy: 0.3572\n",
      "Epoch 46/100\n",
      "375/375 [==============================] - 325s 868ms/step - loss: 2.7177 - accuracy: 0.3679\n",
      "Epoch 47/100\n",
      "375/375 [==============================] - 326s 870ms/step - loss: 2.6743 - accuracy: 0.3735\n",
      "Epoch 48/100\n",
      "375/375 [==============================] - 327s 872ms/step - loss: 2.6173 - accuracy: 0.3850\n",
      "Epoch 49/100\n",
      "375/375 [==============================] - 325s 867ms/step - loss: 2.5601 - accuracy: 0.3949\n",
      "Epoch 50/100\n",
      "375/375 [==============================] - 323s 862ms/step - loss: 2.5143 - accuracy: 0.4046\n",
      "Epoch 51/100\n",
      "375/375 [==============================] - 325s 867ms/step - loss: 2.4677 - accuracy: 0.4131\n",
      "Epoch 52/100\n",
      "375/375 [==============================] - 324s 864ms/step - loss: 2.4113 - accuracy: 0.4235\n",
      "Epoch 53/100\n",
      "375/375 [==============================] - 325s 866ms/step - loss: 2.3640 - accuracy: 0.4349\n",
      "Epoch 54/100\n",
      "375/375 [==============================] - 325s 866ms/step - loss: 2.3187 - accuracy: 0.4420\n",
      "Epoch 55/100\n",
      "375/375 [==============================] - 324s 864ms/step - loss: 2.2712 - accuracy: 0.4507\n",
      "Epoch 56/100\n",
      "375/375 [==============================] - 325s 866ms/step - loss: 2.2216 - accuracy: 0.4629\n",
      "Epoch 57/100\n",
      "375/375 [==============================] - 327s 872ms/step - loss: 2.1715 - accuracy: 0.4714\n",
      "Epoch 58/100\n",
      "375/375 [==============================] - 328s 874ms/step - loss: 2.1234 - accuracy: 0.4802\n",
      "Epoch 59/100\n",
      "375/375 [==============================] - 333s 888ms/step - loss: 2.0738 - accuracy: 0.4936\n",
      "Epoch 60/100\n",
      "375/375 [==============================] - 340s 907ms/step - loss: 2.0257 - accuracy: 0.5013\n",
      "Epoch 61/100\n",
      "375/375 [==============================] - 333s 888ms/step - loss: 1.9832 - accuracy: 0.5105\n",
      "Epoch 62/100\n",
      "375/375 [==============================] - 331s 882ms/step - loss: 1.9277 - accuracy: 0.5227\n",
      "Epoch 63/100\n",
      "375/375 [==============================] - 333s 889ms/step - loss: 1.8835 - accuracy: 0.5336\n",
      "Epoch 64/100\n",
      "375/375 [==============================] - 330s 879ms/step - loss: 1.8429 - accuracy: 0.5416\n",
      "Epoch 65/100\n",
      "375/375 [==============================] - 331s 882ms/step - loss: 1.7946 - accuracy: 0.5530\n",
      "Epoch 66/100\n",
      "375/375 [==============================] - 329s 877ms/step - loss: 1.7463 - accuracy: 0.5627\n",
      "Epoch 67/100\n",
      "375/375 [==============================] - 329s 877ms/step - loss: 1.7019 - accuracy: 0.5722\n",
      "Epoch 68/100\n",
      "375/375 [==============================] - 331s 883ms/step - loss: 1.6518 - accuracy: 0.5858\n",
      "Epoch 69/100\n",
      "375/375 [==============================] - 332s 886ms/step - loss: 1.6039 - accuracy: 0.5978\n",
      "Epoch 70/100\n",
      "375/375 [==============================] - 339s 904ms/step - loss: 1.5527 - accuracy: 0.6082\n",
      "Epoch 71/100\n",
      "375/375 [==============================] - 326s 869ms/step - loss: 1.5119 - accuracy: 0.6175\n",
      "Epoch 72/100\n",
      "375/375 [==============================] - 327s 873ms/step - loss: 1.4737 - accuracy: 0.6252\n",
      "Epoch 73/100\n",
      "375/375 [==============================] - 325s 867ms/step - loss: 1.4163 - accuracy: 0.6421\n",
      "Epoch 74/100\n",
      "375/375 [==============================] - 324s 865ms/step - loss: 1.3718 - accuracy: 0.6516\n",
      "Epoch 75/100\n",
      "375/375 [==============================] - 329s 877ms/step - loss: 1.3278 - accuracy: 0.6640\n",
      "Epoch 76/100\n",
      "375/375 [==============================] - 327s 873ms/step - loss: 1.2827 - accuracy: 0.6752\n",
      "Epoch 77/100\n",
      "375/375 [==============================] - 326s 870ms/step - loss: 1.2377 - accuracy: 0.6833\n",
      "Epoch 78/100\n",
      "375/375 [==============================] - 327s 871ms/step - loss: 1.2020 - accuracy: 0.6923\n",
      "Epoch 79/100\n",
      "375/375 [==============================] - 327s 873ms/step - loss: 1.1545 - accuracy: 0.7053\n",
      "Epoch 80/100\n",
      "375/375 [==============================] - 327s 872ms/step - loss: 1.1137 - accuracy: 0.7159\n",
      "Epoch 81/100\n",
      "375/375 [==============================] - 324s 863ms/step - loss: 1.0654 - accuracy: 0.7287\n",
      "Epoch 82/100\n",
      "375/375 [==============================] - 324s 864ms/step - loss: 1.0186 - accuracy: 0.7414\n",
      "Epoch 83/100\n",
      "375/375 [==============================] - 310s 826ms/step - loss: 0.9781 - accuracy: 0.7533\n",
      "Epoch 84/100\n",
      "375/375 [==============================] - 309s 825ms/step - loss: 0.9458 - accuracy: 0.7583\n",
      "Epoch 85/100\n",
      "375/375 [==============================] - 309s 825ms/step - loss: 0.9052 - accuracy: 0.7695\n",
      "Epoch 86/100\n",
      "375/375 [==============================] - 311s 830ms/step - loss: 0.8679 - accuracy: 0.7789\n",
      "Epoch 87/100\n",
      "375/375 [==============================] - 318s 848ms/step - loss: 0.8282 - accuracy: 0.7898\n",
      "Epoch 88/100\n",
      "375/375 [==============================] - 311s 830ms/step - loss: 0.7873 - accuracy: 0.8028\n",
      "Epoch 89/100\n",
      "375/375 [==============================] - 304s 811ms/step - loss: 0.7554 - accuracy: 0.8084\n",
      "Epoch 90/100\n",
      "375/375 [==============================] - 299s 797ms/step - loss: 0.7245 - accuracy: 0.8178\n",
      "Epoch 91/100\n",
      "375/375 [==============================] - 299s 798ms/step - loss: 0.6774 - accuracy: 0.8310\n",
      "Epoch 92/100\n",
      "375/375 [==============================] - 302s 804ms/step - loss: 0.6395 - accuracy: 0.8418\n",
      "Epoch 93/100\n",
      "375/375 [==============================] - 300s 801ms/step - loss: 0.6285 - accuracy: 0.8416\n",
      "Epoch 94/100\n",
      "375/375 [==============================] - 301s 803ms/step - loss: 0.6062 - accuracy: 0.8482\n",
      "Epoch 95/100\n",
      "375/375 [==============================] - 303s 807ms/step - loss: 0.5673 - accuracy: 0.8607\n",
      "Epoch 96/100\n",
      "375/375 [==============================] - 301s 801ms/step - loss: 0.5461 - accuracy: 0.8645\n",
      "Epoch 97/100\n",
      "375/375 [==============================] - 301s 802ms/step - loss: 0.5125 - accuracy: 0.8743\n",
      "Epoch 98/100\n",
      "375/375 [==============================] - 300s 800ms/step - loss: 0.4848 - accuracy: 0.8809\n",
      "Epoch 99/100\n",
      "375/375 [==============================] - 301s 802ms/step - loss: 0.4581 - accuracy: 0.8902\n",
      "Epoch 100/100\n",
      "375/375 [==============================] - 302s 805ms/step - loss: 0.4319 - accuracy: 0.8954\n"
     ]
    }
   ],
   "source": [
    "# Compiling and fitting model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history= model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cacba0c4-c444-4b62-8ac2-fe02ce5158fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained model\n",
    "model.save('C:/Users/kapil/Desktop/NLP.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de7aa77e-fbba-4c47-a836-d0a8efd0ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving tokeniser\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b01dcb7-58bd-4dd5-bc16-c90745c07c9e",
   "metadata": {},
   "source": [
    "### Testing on Unknown text/ words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56ba7bfa-c79f-42d1-8ad8-873b54d6aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model and tokeniser\n",
    "model = load_model(\"C:/Users/Aditi Sony/Downloads/Deep Learning/Project/NLP/NLP.h5\")\n",
    "tokenizer = load(open(\"C:/Users/Aditi Sony/Downloads/Deep Learning/Project/NLP/tokenizer.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9e21c94-76e6-4bf3-afd5-648f12f3232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test words\n",
    "test_text= '''I do not know whether Papa guessed my feelings on the subject,\n",
    "probably not, and in any case he would not have been interested.\n",
    "The opinion of other people never interested him in the slightest\n",
    "degree. I think it was really a sign of his greatness. In the same\n",
    "way, he lived quite detached from the necessities of daily life.\n",
    "He ate what was put before him in an exemplary fashion, but seemed\n",
    "mildly pained when the question of paying\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0d4730b-25c9-4b62-afa4-f6be804f3d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I do not know whether Papa guessed my feelings on the subject,\n",
      "probably not, and in any case he would not have been interested.\n",
      "The opinion of other people never interested him in the slightest\n",
      "degree. I think it was really a sign of his greatness. In the same\n",
      "way, he lived quite detached from the necessities of daily life.\n",
      "He ate what was put before him in an exemplary fashion, but seemed\n",
      "mildly pained when the question of paying\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "61716691-5b5e-496c-8470-462b1fee24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning text\n",
    "def clean_text(text):\n",
    "    # replace '--' with a space\n",
    "    text= text.replace('--', ' ')\n",
    "    #splitting into tokens by space\n",
    "    tokens= text.split()\n",
    "    #removing punctuations\n",
    "    tokens= [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "    #removing non-aplhabetic tokens\n",
    "    tokens= [word for word in tokens if word.isalpha()]\n",
    "    # lower-case\n",
    "    tokens= [word.lower() for word in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9e2b84b9-be84-4106-b098-419c623c18ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "61\n"
     ]
    }
   ],
   "source": [
    "tokens_test= clean_text(test_text)\n",
    "print(len(tokens_test))\n",
    "print(len(set(tokens_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c2adace6-287c-4b06-a03a-840dc2f4100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating text\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        yhat = np.argmax(model.predict(encoded, verbose=0), axis= -1)\n",
    "        # map predicted word index to word\n",
    "        out_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == yhat:\n",
    "                out_word = word\n",
    "                break\n",
    "        # append to input\n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45d98fb1-6025-40e9-861e-f3d6df6e842e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "was that the weapon was in every small gentleman too dying by a thick moustache i was able to mean i writes must have done just to come at and rifled a chink since five husband was develops prove she appeared to straighten things up and then again for any\n"
     ]
    }
   ],
   "source": [
    "# generate new text\n",
    "seq_length = 10\n",
    "generated = generate_seq(model, tokenizer, seq_length, test_text, 50)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095fc215-9e10-4729-af1c-e1286ae3ea6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
